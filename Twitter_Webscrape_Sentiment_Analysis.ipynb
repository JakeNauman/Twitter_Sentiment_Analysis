{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter_Webscrape_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9QEyz3tQdYWFcL2cxkZH1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakeNauman/Twitter_Sentiment_Analysis/blob/main/Twitter_Webscrape_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Twitter live webscraping sentiment analysis project**"
      ],
      "metadata": {
        "id": "FnHNyPnst6fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction      \n",
        "In this project, I chose to use sentiment analysis classification on Twitter tweet text, just like we did in class. However, I took it one step further and used Twitter developer api to return the most recent tweets on a user given topic. I was able to obtain real data which made the program much more useful and practical. By combining this with what we learned in class, I will be able to determine the public sentiment at any given moment on any subject given by the user by running sentiment analysis on the most recent 100 or so tweets that mention that subject. For example, if the user inputs 'Netflix' the algorithm will search 100 recent tweets mentioning Netflix and calculate if it is a generally positive or negative subject. As a result it will most likely show that Netflix is viewed negatively at the moment."
      ],
      "metadata": {
        "id": "1csmoNvoCw53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get access to the Twitter api you have to apply to be a developer and get an access code, but as I dont plan to use Twitter developer tools any further I'll just leave my code in so it will run. Also, it may return innapropriate results because it is using live tweets, and Twitter is a gross place so be warned. At the moment, Twitter only allows you to obtain 100 of the most recent tweets from their data to avoid strain on servers, but that is more than enough for my needs."
      ],
      "metadata": {
        "id": "Jzupb4rbDXaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is the Twitter code for webscraping twitter data. The function takes in information for what to return of the Twitter data such as fields, search topic, and bearer token which authorizes access. It then forms the URL from which it gathers and returns the specified data in the form of a list."
      ],
      "metadata": {
        "id": "rOcvN_85D-HI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXwQaj2zdlH2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "#This is my Twitter developer token. I cant include this but it's easy to get one yourself.\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAPj6cgEAAAAA0OnV14VJfxRQzvrUaERxWp8DYMg%3Dc4FnO8mYBEMPDrhiq4Zco4zhTFxdLJy9kfLGNmoGFUtbBDS8yI\"\n",
        "\n",
        "#define search twitter function\n",
        "def search_twitter(query, tweet_fields,bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "\n",
        "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}\".format(\n",
        "        query, 'max_results=100', tweet_fields\n",
        "    )\n",
        "    response = requests.request(\"GET\", url, headers=headers)\n",
        "\n",
        "    print()\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is the sentiment analysis algorithm we did in class. This code trains the program to classify key words as positive or negative in order to use that in sentances to decifer + or - sentiment. It uses the NLTK  API to obtain sample twitter text along with a + or - classification which it uses to train off of."
      ],
      "metadata": {
        "id": "BqZPRBZEpd3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentiment Analysis: \")\n",
        "\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('twitter_samples')\n",
        "#import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "#import io\n",
        "#import unicodedata\n",
        "#import numpy as np\n",
        "#import re\n",
        "#import string\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "from nltk.corpus import twitter_samples as tw\n",
        "\n",
        "# Tweets with positive sentiment ---> 5000\n",
        "pos_tw = tw.strings('positive_tweets.json')\n",
        "# Tweets with negative sentment ---> 5000\n",
        "neg_tw = tw.strings('negative_tweets.json')\n",
        "# Tweets with no polarity ---> 20000\n",
        "txt = tw.strings('tweets.20150430-223406.json')\n",
        "\n",
        "pos_token = tw.tokenized('positive_tweets.json')\n",
        "neg_token = tw.tokenized('negative_tweets.json')\n",
        "#print(pos_tw)\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#normalize data\n",
        "from nltk.tag import pos_tag\n",
        "print(pos_tag(pos_token[0]))\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#word lettemizer to normalize tweet data\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "def lemmatize_sentence(tokens):\n",
        "  lem = WordNetLemmatizer()\n",
        "  lem_sent = []\n",
        "  for word, tag in pos_tag(tokens):\n",
        "    if tag.startswith('NN'):\n",
        "      pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "      pos = 'v'\n",
        "    else:\n",
        "      pos = 'a'\n",
        "    lem_sent.append(lem.lemmatize(word, pos))\n",
        "  return lem_sent\n",
        "print(lemmatize_sentence(pos_token[0]))\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#remove special characters with regular expression\n",
        "import re, string\n",
        "def noise_removal(twt_tokens, stop_words = ()):\n",
        "  cleaned_data = []\n",
        "  for token, tag in pos_tag(twt_tokens):\n",
        "    token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "    '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "    token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "    if tag.startswith(\"NN\"):\n",
        "      pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "      pos = 'v'\n",
        "    else:\n",
        "      pos = 'a'\n",
        "    lem = WordNetLemmatizer()\n",
        "    token = lem.lemmatize(token, pos)\n",
        "    if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "      cleaned_data.append(token.lower())\n",
        "  return cleaned_data\n",
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('english')\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#remove noise from pos and neg tweets\n",
        "pos_cleaned_data = []\n",
        "neg_cleaned_data = []\n",
        "\n",
        "for token in pos_token:\n",
        "  pos_cleaned_data.append(noise_removal(token,sw))\n",
        "for token in neg_token:\n",
        "  neg_cleaned_data.append(noise_removal(token,sw))\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#determine word density in data\n",
        "def make_word_list(cleaned_list):\n",
        "  for words in cleaned_list:\n",
        "    for word in words:\n",
        "      yield word\n",
        "pos_words = make_word_list(pos_cleaned_data)\n",
        "neg_words = make_word_list(neg_cleaned_data)\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#find most common words using frequency distribution\n",
        "from nltk import FreqDist\n",
        "pos_word_freq=FreqDist(pos_words)\n",
        "neg_word_freq=FreqDist(neg_words)\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#prep data for analysing module\n",
        "\n",
        "def prepare_data_for_model(data_set):\n",
        "  for tokens in data_set:\n",
        "    yield dict([token,True] for token in tokens)\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#prep both neg and pos tagged words separately\n",
        "pos_token_model = prepare_data_for_model(pos_cleaned_data)\n",
        "neg_token_model = prepare_data_for_model(neg_cleaned_data)\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#randomize the dataset\n",
        "import random\n",
        "pos_dataset = [(twt_dict,'positive') for twt_dict in pos_token_model]\n",
        "neg_dataset = [(twt_dict,'negative') for twt_dict in neg_token_model]\n",
        "\n",
        "dataset = pos_dataset + neg_dataset\n",
        "random.shuffle(dataset)\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "\n",
        "#training modules using classifiers\n",
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier as NB\n",
        "classifier_NB = NB.train(train_data)\n",
        "\n",
        "print(\"Accuracy: \", classify.accuracy(classifier_NB,test_data))\n",
        "\n",
        "print(classifier_NB.show_most_informative_features(10))\n",
        "\n",
        "#___________________________________________________________________________________________________________________\n",
        "print(\"Algorithm Trained\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd_kvw2vEyWF",
        "outputId": "787767de-98e1-4e4f-f816-58c7b6193938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis: \n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n",
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n",
            "Accuracy:  0.9963333333333333\n",
            "Most Informative Features\n",
            "                      :( = True           negati : positi =   2053.8 : 1.0\n",
            "                      :) = True           positi : negati =   1645.8 : 1.0\n",
            "                follower = True           positi : negati =     22.4 : 1.0\n",
            "                     sad = True           negati : positi =     19.5 : 1.0\n",
            "                     x15 = True           negati : positi =     19.5 : 1.0\n",
            "                     bam = True           positi : negati =     18.5 : 1.0\n",
            "               community = True           positi : negati =     15.8 : 1.0\n",
            "                 welcome = True           positi : negati =     14.6 : 1.0\n",
            "                    glad = True           positi : negati =     14.3 : 1.0\n",
            "                  arrive = True           positi : negati =     14.3 : 1.0\n",
            "None\n",
            "Algorithm Trained\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got the 100 or so most recent tweets on that particular subject and we've got the algorithm trained, we can combine them and analyze results. The code below will go through every tweet and decide if it is positive or negative. At the end there is an overview of the results. This is the main code to run once the top two code blocks have been run and loaded."
      ],
      "metadata": {
        "id": "nMN8qmtrrjmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Enter a search topic: \")\n",
        "\n",
        "#twitter fields to be returned by api call\n",
        "tweet_fields = \"tweet.fields=text,author_id,created_at\"\n",
        "\n",
        "\n",
        "#twitter api call\n",
        "json_response = search_twitter(query=query, tweet_fields=tweet_fields, bearer_token=bearer_token)\n",
        "tweets = json_response['data']\n",
        "listoftweets=[]\n",
        "for i in tweets:\n",
        "  listoftweets.append(i['text'])\n",
        "print(str(len(listoftweets)) + \" of the most recent Tweets mentioning '\" + str(query) + \"' scraped for analyzing.\")\n",
        "#This will print all tweet info, I only need the text\n",
        "#print(json.dumps(json_response['data'], indent=4, sort_keys=True))\n",
        "\n",
        "from nltk.grammar import ProbabilisticProduction\n",
        "#check classifier w tweets\n",
        "from nltk.tokenize import word_tokenize as wt\n",
        "sentimentlist=[]\n",
        "for i in listoftweets:\n",
        "  custom_tokens = noise_removal(wt(i),sw)\n",
        "  #print(i) #show every tweet and if its pos or neg\n",
        "\n",
        "  sentimentlist.append(classifier_NB.classify(dict([token,True] for token in custom_tokens)))\n",
        "  #print(classifier_NB.classify(dict([token,True] for token in custom_tokens))) #print if each tweet is pos or neg\n",
        "\n",
        "\n",
        "#calculate sentiment\n",
        "poscount=0\n",
        "negcount=0\n",
        "for i in sentimentlist:\n",
        "  if i=='positive':\n",
        "    poscount+=1\n",
        "  else:\n",
        "    negcount+=1\n",
        "  \n",
        "\n",
        "\n",
        "print(\"positive tweets: \" + str(poscount))\n",
        "print(\"negative tweets: \" + str(negcount))\n",
        "\n",
        "\n",
        "\n",
        "if poscount>negcount:\n",
        "  print(\"The topic of '\" + str(query) + \"' is mostly positive\")\n",
        "elif poscount<negcount:\n",
        "  print(\"The topic of '\" + str(query) + \"' is mostly negative\")\n",
        "else:\n",
        "  print(\"The topic of '\" + str(query) + \"' is neutral\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVOFITVsCMyR",
        "outputId": "ec6ca188-54a1-469c-afa7-e162b4328e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a search topic: school\n",
            "\n",
            "100 of the most recent Tweets mentioning 'school' scraped for analyzing.\n",
            "positive tweets: 30\n",
            "negative tweets: 70\n",
            "The topic of 'school' is mostly negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some interesting results\n",
        "\n",
        "\n",
        "\n",
        "Positive Topics\n",
        "*   'Birthday' was 90:10  mostly positive\n",
        "*   'The economy' was 72:28  mostly positive (suprisingly)\n",
        "*   'Basketball' was 70:30  mostly positive\n",
        "*   'Graduation' was 58:42  mostly positive\n",
        "*   'Stranger Things' was 61:39  mostly positive\n",
        "*   'Minnetonka' was 71:29  mostly positive\n",
        "\n",
        "Neutral Topics (rare)\n",
        "*   'United States' was 50:50 split\n",
        "\n",
        "\n",
        "Negative Topics\n",
        "*   'Funeral' was 17:83  mostly negative\n",
        "*   'Ukraine' was 40:60 mostly negative\n",
        "*   'Netflix' was 15:85  mostly negative\n",
        "*   'School' was 42:58  mostly negative\n",
        "*   'Nike' was 39:61  mostly negative\n",
        "*   'Joe Biden' was 45:55  mostly negative\n",
        "*   'Morbius' was 39:61  mostly negative\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JpxEWnSiuvnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm is pretty accurate for the most part. Most of the results fit the sentiment that you would expect. For example, Netflix is losing popularity at the moment and that fits the mostly negative public feeling. Also 'birthday' you would expect to be positive because it is a happy topic. From these and other examples it is very liekly that this is a useful tool to determine public sentiment on any given subject. However, when running the twitter code I found that a lot of the returned recent tweets are bots that mess up the results quite a bit by spamming the same message. Also some results are simply unexpected, like how 'the economy' is mostly positive while all the stocks are dropping at the moment. It could return positive for other, unknown reasons though. However it still works as intended and maybe taking in a larger amount of data would help."
      ],
      "metadata": {
        "id": "Z6k3_p-Ivvce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report\n",
        "\n",
        "Introduction\n",
        "\n",
        "For this project I combined the data I accessed through Twitter developer api with sentiment analysis machine learning to be able to determine the community sentiment on any user given topic. For this project I learned that Twitter gives access to certain amounts of data for personal use through their own api which I had to apply for. Using this you can obtain any data you like. In this case I need a list of the text of the most recent tweets to mention a specific keyword given by the user. This sentiment analysis project did use a lot of similar code to what we did in class, but I'm proud I was able to apply it to actual data in real time to calculate overall sentiment. It uses NLTK API to obtain sample twitter data which it uses to train the algorithm to classify given text as positive or negative. Through this, I can classify sentiment of real time tweets and see how many it predicts are positive and negative.\n",
        "\n",
        "How it works\n",
        "\n",
        "It starts off by asking the user for input, the 'query' that needs to be included in the twitter api call to obtain data. Then it combines that with the desired data (in this case only the tweet text) and the bearer token to authorize that the user has access to the call which I obtained just for this project. This is to make sure that whoever requests the data is a verified user who Twitter can track their usage. This exists so crypto bots and whatnot can't constantly take in data and use it for personal gain. AFter given the specified requirements, it returns a specified number of recent tweets that contain the 'query' which I add to a list called tweets. Twitter lets you obtain anywhere from 10-100 tweets for any purpose. You can also specify what data you want (time posted, username, likes, comments...) but the text is obviously the most important component when determining sentiment.\n",
        "\n",
        " Then, the code from the machine learning sentiment analysis program is used. This code takes in sample data from NLTK that make up thousands of fake tweets with a specified sentiment. It tokenizes and then scans all the words and decides which words and phrases are most associated with positive or negative tweets. It gets rid of stop words such as (or, the, and a...) so it can only use the notable and important words to associate with a sentiment. Once the algorithm is trained, the user can then determine accuracy and test user given tweets.\n",
        "\n",
        " Finally, these two programs can be combined. If we run the list of tweets through a for loop performing the sentiment analysis classifier and make a new list containing around 100 values of either 'positive' or 'negative'. Then I used some simple python to find the ratio of + to - and decide whether the results were mostly positive, mostly negative, or the same. \n",
        "\n",
        " Results\n",
        "\n",
        " To the most part, these results were pretty accurate. Even if the classifier classifies a few tweets as the wrong sentiment, 100 tweets are more than enough to even out the data and give you an accurate portrayal. I know that this algorithm is somewhat reliable for a variety of specific reasons relating to current events and general public knowledge. Meaning that the result fits what I expected it to do.\n",
        " 1. Due to increasing unrest, the topic of Joe Biden would be expected to be mostly negative, which the algorithm mirrors.\n",
        " 2. There is a current rise in unpopularity in Netflix as its stock plummets and many people choosing to unsubscribe. My data reflects the mostly negative sentiment\n",
        " 3. Keywords such as 'birthday', 'graduation', and 'basketball' all return mostly positive which fits with the positive relations to the word.\n",
        " 4. With the war happening in Ukraine, the algorithm would most likely return a mostly negative sentiment  assesment which it does.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "In conclusion, this project is very successful and I was able to obtain real data to create something that is actually practical and could be used for a variety of things. This could be used for companies to determine social media's sentiment on their brand. It could also be used for personal use to judge Twitter opinions on a particular product by inputting, say, 'MacBook Pro' and seeing what 100 people think of it. I could maybe imrove this project by obtaining more data, or making it even more practical by obtaining Facebook data, Instagram data, Reddit data and comaring them. If Twitter allowed more than 100 tweets for analysis the results would be more accurate. I could also use more sample tweets to train the algorithm to perform more accurately, but that would take a lot more time and I couldn't find any other data. This was a useful project to learn sentiment analysis and combine it to real world data.\n"
      ],
      "metadata": {
        "id": "h51IYQhjFGxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources\n",
        "\n",
        "Twitter Webscrape Tutorial\n",
        "AuthorPiyushPiyush is a data scientist passionate about using data to understand things better and make informed decisions. In the past, et al. “Get Data from Twitter API in Python - Step by Step Guide.” Data Science Parichay, 15 Dec. 2021, https://datascienceparichay.com/article/get-data-from-twitter-api-in-python-step-by-step-guide/. \n",
        "\n",
        "\n",
        "NLTK Sentiment Analysis Tutorial\n",
        "“NLTK Sentiment Analysis Tutorial: Text Mining &amp; Analysis in Python.” DataCamp, https://www.datacamp.com/tutorial/text-analytics-beginners-nltk?irclickid=zfvTeUVYFxyIUzuxFTRRGWYMUkDxiQ3bhXd-Rc0&amp;irgwc=1&amp;utm_medium=affiliate&amp;utm_source=impact&amp;utm_campaign=2003851.\n",
        "\n",
        "\n",
        "Twitter data \n",
        "Twitter.com\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FRaahk4Sgpiq"
      }
    }
  ]
}